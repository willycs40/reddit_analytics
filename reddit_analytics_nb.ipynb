{
 "metadata": {
  "name": "",
  "signature": "sha256:2976dfe2118870b5392eda2df18855a0bdebf8531d529c771905fc8495815897"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import praw\n",
      "import datetime\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_agent = 'User-Agent: windows:reddit_analytics:v1 (by /u/willycs40)'\n",
      "r = praw.Reddit(user_agent=user_agent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_dt = datetime.datetime(2014,8,1)\n",
      "end_dt = datetime.datetime(2015,8,14)\n",
      "day_delta = datetime.timedelta(days=10)\n",
      "\n",
      "timespans = []\n",
      "current_dt = start_dt\n",
      "while current_dt <= end_dt:\n",
      "    timespans.append(current_dt)\n",
      "    current_dt += day_delta\n",
      "\n",
      "epochs = [int((x - datetime.datetime(1970,1,1)).total_seconds()) for x in timespans]\n",
      "epochs = zip(epochs[0:-1],epochs[1:])\n",
      "print('{} epochs in total.'.format(len(epochs)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "37\n",
        "[(1406851200, 1407715200), (1407715200, 1408579200), (1408579200, 1409443200), (1409443200, 1410307200), (1410307200, 1411171200), (1411171200, 1412035200), (1412035200, 1412899200), (1412899200, 1413763200), (1413763200, 1414627200), (1414627200, 1415491200), (1415491200, 1416355200), (1416355200, 1417219200), (1417219200, 1418083200), (1418083200, 1418947200), (1418947200, 1419811200), (1419811200, 1420675200), (1420675200, 1421539200), (1421539200, 1422403200), (1422403200, 1423267200), (1423267200, 1424131200), (1424131200, 1424995200), (1424995200, 1425859200), (1425859200, 1426723200), (1426723200, 1427587200), (1427587200, 1428451200), (1428451200, 1429315200), (1429315200, 1430179200), (1430179200, 1431043200), (1431043200, 1431907200), (1431907200, 1432771200), (1432771200, 1433635200), (1433635200, 1434499200), (1434499200, 1435363200), (1435363200, 1436227200), (1436227200, 1437091200), (1437091200, 1437955200), (1437955200, 1438819200)]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_submissions(submissions):\n",
      "    '''A method to take a set of submissions and return of lists of lists with selected fields only'''\n",
      "    posts = []\n",
      "    for post in submissions:\n",
      "        all_posts.append([post.id\n",
      "               , post.title\n",
      "               , post.created_utc\n",
      "               , post.author\n",
      "               , post.url\n",
      "               , post.domain\n",
      "               , post.permalink\n",
      "               , post.ups\n",
      "               , post.downs\n",
      "               , post.score\n",
      "               , post.is_self\n",
      "               , post.num_comments\n",
      "               , post.subreddit\n",
      "               , post.thumbnail\n",
      "               ])\n",
      "    return posts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_posts = []\n",
      "for epoch in epochs:\n",
      "    query = 'timestamp:{}..{}'.format(epoch[0], epoch[1]-1)\n",
      "    print(query)\n",
      "    submissions = r.search(query, subreddit='machinelearning', sort='new', limit=100, syntax='cloudsearch')\n",
      "    all_posts.extend(process_submissions(submissions))\n",
      "print('{} posts retrieved in total'.format(len(all_posts)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "timestamp:1406851200..1407715199\n",
        "timestamp:1407715200..1408579199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1408579200..1409443199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1409443200..1410307199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1410307200..1411171199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1411171200..1412035199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1412035200..1412899199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1412899200..1413763199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1413763200..1414627199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1414627200..1415491199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1415491200..1416355199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1416355200..1417219199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1417219200..1418083199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1418083200..1418947199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1418947200..1419811199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1419811200..1420675199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1420675200..1421539199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1421539200..1422403199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1422403200..1423267199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1423267200..1424131199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1424131200..1424995199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1424995200..1425859199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1425859200..1426723199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1426723200..1427587199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1427587200..1428451199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1428451200..1429315199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1429315200..1430179199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1430179200..1431043199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1431043200..1431907199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1431907200..1432771199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1432771200..1433635199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1433635200..1434499199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1434499200..1435363199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1435363200..1436227199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1436227200..1437091199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1437091200..1437955199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "timestamp:1437955200..1438819199"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "c:/Users/will/Documents/Projects/reddit_analytics/venv\\Lib\\site-packages\\requests\\packages\\urllib3\\util\\ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n",
        "  InsecurePlatformWarning\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Convert list to dataframe, convert created epoch to date, simplify thumbnail, create a title length and is_question columns"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.DataFrame(all_posts, columns=['id','title','created','author','url','domain','permalink','ups','downs','score', 'is_self', 'num_comments','subreddit', 'thumbnail'])\n",
      "df['thumbnail'] = df['thumbnail'].apply(lambda x: False if x in ['self','default'] else True)\n",
      "df['created'] = df['created'].apply(lambda x: datetime.datetime(1970,1,1) + datetime.timedelta(seconds=x))\n",
      "df['title_length'] = df['title'].apply(lambda x: len(x))\n",
      "df['is_question'] =  df['title'].apply(lambda x: True if '?' in x else False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>id</th>\n",
        "      <th>title</th>\n",
        "      <th>created</th>\n",
        "      <th>author</th>\n",
        "      <th>url</th>\n",
        "      <th>domain</th>\n",
        "      <th>permalink</th>\n",
        "      <th>ups</th>\n",
        "      <th>downs</th>\n",
        "      <th>score</th>\n",
        "      <th>is_self</th>\n",
        "      <th>num_comments</th>\n",
        "      <th>subreddit</th>\n",
        "      <th>thumbnail</th>\n",
        "      <th>title_length</th>\n",
        "      <th>is_question</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>2d5io0</td>\n",
        "      <td>Resources for learning about ensemble learning?</td>\n",
        "      <td>2014-08-10 15:45:08</td>\n",
        "      <td>rovingr</td>\n",
        "      <td>http://www.reddit.com/r/MachineLearning/commen...</td>\n",
        "      <td>self.MachineLearning</td>\n",
        "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
        "      <td>11</td>\n",
        "      <td>0</td>\n",
        "      <td>11</td>\n",
        "      <td>True</td>\n",
        "      <td>10</td>\n",
        "      <td>MachineLearning</td>\n",
        "      <td>False</td>\n",
        "      <td>47</td>\n",
        "      <td>True</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>2d4y8z</td>\n",
        "      <td>how cross validation help to gain a better model?</td>\n",
        "      <td>2014-08-10 09:44:01</td>\n",
        "      <td>phoenixbai</td>\n",
        "      <td>http://www.reddit.com/r/MachineLearning/commen...</td>\n",
        "      <td>self.MachineLearning</td>\n",
        "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>True</td>\n",
        "      <td>12</td>\n",
        "      <td>MachineLearning</td>\n",
        "      <td>False</td>\n",
        "      <td>49</td>\n",
        "      <td>True</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>2czt7i</td>\n",
        "      <td>Machine Learning Theory: An Introductory Primer</td>\n",
        "      <td>2014-08-08 17:00:53</td>\n",
        "      <td>hs613</td>\n",
        "      <td>http://www.toptal.com/machine-learning/machine...</td>\n",
        "      <td>toptal.com</td>\n",
        "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
        "      <td>51</td>\n",
        "      <td>0</td>\n",
        "      <td>51</td>\n",
        "      <td>False</td>\n",
        "      <td>5</td>\n",
        "      <td>MachineLearning</td>\n",
        "      <td>True</td>\n",
        "      <td>47</td>\n",
        "      <td>False</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>2czekz</td>\n",
        "      <td>Using scikit-learn Pipelines and FeatureUnions</td>\n",
        "      <td>2014-08-08 14:41:59</td>\n",
        "      <td>eloisius</td>\n",
        "      <td>http://zacstewart.com/2014/08/05/pipelines-of-...</td>\n",
        "      <td>zacstewart.com</td>\n",
        "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
        "      <td>15</td>\n",
        "      <td>0</td>\n",
        "      <td>15</td>\n",
        "      <td>False</td>\n",
        "      <td>0</td>\n",
        "      <td>MachineLearning</td>\n",
        "      <td>True</td>\n",
        "      <td>46</td>\n",
        "      <td>False</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>2czdp2</td>\n",
        "      <td>Implementation of Developmental Learning MOOC ...</td>\n",
        "      <td>2014-08-08 14:33:25</td>\n",
        "      <td>alexgmcm</td>\n",
        "      <td>http://liris.cnrs.fr/ideal/mooc/</td>\n",
        "      <td>liris.cnrs.fr</td>\n",
        "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>False</td>\n",
        "      <td>5</td>\n",
        "      <td>MachineLearning</td>\n",
        "      <td>True</td>\n",
        "      <td>70</td>\n",
        "      <td>False</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "       id                                              title  \\\n",
        "0  2d5io0    Resources for learning about ensemble learning?   \n",
        "1  2d4y8z  how cross validation help to gain a better model?   \n",
        "2  2czt7i    Machine Learning Theory: An Introductory Primer   \n",
        "3  2czekz     Using scikit-learn Pipelines and FeatureUnions   \n",
        "4  2czdp2  Implementation of Developmental Learning MOOC ...   \n",
        "\n",
        "              created      author  \\\n",
        "0 2014-08-10 15:45:08     rovingr   \n",
        "1 2014-08-10 09:44:01  phoenixbai   \n",
        "2 2014-08-08 17:00:53       hs613   \n",
        "3 2014-08-08 14:41:59    eloisius   \n",
        "4 2014-08-08 14:33:25    alexgmcm   \n",
        "\n",
        "                                                 url                domain  \\\n",
        "0  http://www.reddit.com/r/MachineLearning/commen...  self.MachineLearning   \n",
        "1  http://www.reddit.com/r/MachineLearning/commen...  self.MachineLearning   \n",
        "2  http://www.toptal.com/machine-learning/machine...            toptal.com   \n",
        "3  http://zacstewart.com/2014/08/05/pipelines-of-...        zacstewart.com   \n",
        "4                   http://liris.cnrs.fr/ideal/mooc/         liris.cnrs.fr   \n",
        "\n",
        "                                           permalink  ups  downs  score  \\\n",
        "0  https://www.reddit.com/r/MachineLearning/comme...   11      0     11   \n",
        "1  https://www.reddit.com/r/MachineLearning/comme...    0      0      0   \n",
        "2  https://www.reddit.com/r/MachineLearning/comme...   51      0     51   \n",
        "3  https://www.reddit.com/r/MachineLearning/comme...   15      0     15   \n",
        "4  https://www.reddit.com/r/MachineLearning/comme...    1      0      1   \n",
        "\n",
        "  is_self  num_comments        subreddit thumbnail  title_length is_question  \n",
        "0    True            10  MachineLearning     False            47        True  \n",
        "1    True            12  MachineLearning     False            49        True  \n",
        "2   False             5  MachineLearning      True            47       False  \n",
        "3   False             0  MachineLearning      True            46       False  \n",
        "4   False             5  MachineLearning      True            70       False  "
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df.drop(['title','url','domain','permalink','author','subreddit'], axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.to_csv('machinedreaming_posts_full.csv', encoding='utf-8', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\f",
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    }
   ],
   "metadata": {}
  }
 ]
}